{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural Language Toolkit (NLTK): The complete toolkit for all NLP techniques.\n",
    "- spaCy — Industrial strength N LP with Python and Cython.\n",
    "- Gensim — Topic Modelling for Humans\n",
    "- Stanford Core NLP — NLP services and packages by Stanford NLP Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47dc9a96be87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity \n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "### for summarization\n",
    "from gensim.summarization.summarizer import summarize as gensim_summarize \n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: get raw text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDf = pd.read_csv(\"../outputs/step1_bigquery_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>THEMES</th>\n",
       "      <th>DocumentIdentifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190101060000</td>\n",
       "      <td>EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINT...</td>\n",
       "      <td>https://www.daijiworld.com/chan/exclusiveDispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190101061500</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_MAN;ARREST;SOC_GENERALCR...</td>\n",
       "      <td>https://caymannewsservice.com/2018/12/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190101063000</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_LEADER;ENV_NUCLEARPOWER;...</td>\n",
       "      <td>https://www.vesti.bg/tehnologii/bil-gejts-sash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190101061500</td>\n",
       "      <td>ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...</td>\n",
       "      <td>https://www.ajc.com/business/economy/georgia-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190101061500</td>\n",
       "      <td>ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...</td>\n",
       "      <td>https://pv-magazine-usa.com/2018/12/18/breakin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATE                                             THEMES  \\\n",
       "0  20190101060000  EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINT...   \n",
       "1  20190101061500  TAX_FNCACT;TAX_FNCACT_MAN;ARREST;SOC_GENERALCR...   \n",
       "2  20190101063000  TAX_FNCACT;TAX_FNCACT_LEADER;ENV_NUCLEARPOWER;...   \n",
       "3  20190101061500  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
       "4  20190101061500  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
       "\n",
       "                                  DocumentIdentifier  \n",
       "0  https://www.daijiworld.com/chan/exclusiveDispl...  \n",
       "1             https://caymannewsservice.com/2018/12/  \n",
       "2  https://www.vesti.bg/tehnologii/bil-gejts-sash...  \n",
       "3  https://www.ajc.com/business/economy/georgia-p...  \n",
       "4  https://pv-magazine-usa.com/2018/12/18/breakin...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2988, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.daijiworld.com/chan/exclusiveDisplay.aspx?articlesID=4959'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = rawDf.DocumentIdentifier.values[0]\n",
    "url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def get_text(url):\n",
    "    \"\"\"\n",
    "    Func: 1. get raw text from url 2. get summary & keyword from text\n",
    "        Input: url, a link to article\n",
    "        Output: dictionary contains 3 keys, text, summary & keywords\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "\n",
    "        ### parse html file\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "    \n",
    "        return text\n",
    "    except:\n",
    "        print(f'fail to download news from {url}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://caymannewsservice.com/2018/12/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDf.DocumentIdentifier[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZAP, together with its subsidiaries, designs, develops, manufactures, and sells electric and advanced technology vehicles in the United States and internationally. The company offers electric, alternative energy and fuel efficient automobiles and commercial vehicles, motorcycles and scooters, and other forms of personal transportation. ZAP also markets its electric transportation products through its zapworld.com Website. The company was formerly known as ZAPWORLD.COM and changed its name to ZAP in 2001. ZAP was founded in 1994 and is headquartered in Santa Rosa, California.\\n\\nMarketBeat Community Rating for ZAP (OTCMKTS ZAAP)\\n\\nCommunity Ranking: 3.1 out of 5 ( ) Outperform Votes: 80 (Vote Outperform) Underperform Votes: 47 (Vote Underperform) Total Votes: 127\\n\\nMarketBeat\\'s community ratings are surveys of what our community members think about ZAP and other stocks. Vote \"Outperform\" if you believe ZAAP will outperform the S&P 500 over the long term. Vote \"Underperform\" if you believe ZAAP will underperform the S&P 500 over the long term. You may vote once every thirty days.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg = get_text(rawDf.DocumentIdentifier[13])\n",
    "eg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: perform nlp analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Translate <br>\n",
    "\n",
    "#### We notice some of the news is not in English, so we need to translate other language to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_lang(text):\n",
    "    ### translate to english\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        print(f\"language is {language}\")\n",
    "    except:\n",
    "        print(\"Not able to detect language\")\n",
    "        language = \"other\"\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language is en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_lang(eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text summarization <br>\n",
    "#### The reason for text summarization\n",
    "- Avoid noise, since content in news will contains some irrelevant info, perform text summarization will reduce the noise\n",
    "- Improve scalability: Decrease the length of string which could increase scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro of gensim summarization<br>\n",
    "#### The gensim implementation is based on the popular TextRank algorithm. some intro link: https://medium.com/@shivangisareen/text-summarisation-with-gensim-textrank-46bbb3401289, \n",
    "#### There are two kinds of method:\n",
    "- Extractive methods — Involves the selection of phrases and sentences from the source document to make up the new summary.\n",
    "- Abstractive methods- It involves generating entirely new phrases and sentences to capture the meaning of the source document.\n",
    "<br>\n",
    "\n",
    "##### Here we are Extractive methods,using textRank based text summarization,.\n",
    "(to learn more, plz goolgle or contact the DBC consultant for further pure nlp courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZAP, together with its subsidiaries, designs, develops, manufactures, and sells electric and advanced technology vehicles in the United States and internationally. The company offers electric, alternative energy and fuel efficient automobiles and commercial vehicles, motorcycles and scooters, and other forms of personal transportation. ZAP also markets its electric transportation products through its zapworld.com Website. The company was formerly known as ZAPWORLD.COM and changed its name to ZAP in 2001. ZAP was founded in 1994 and is headquartered in Santa Rosa, California.\\n\\nMarketBeat Community Rating for ZAP (OTCMKTS ZAAP)\\n\\nCommunity Ranking: 3.1 out of 5 ( ) Outperform Votes: 80 (Vote Outperform) Underperform Votes: 47 (Vote Underperform) Total Votes: 127\\n\\nMarketBeat\\'s community ratings are surveys of what our community members think about ZAP and other stocks. Vote \"Outperform\" if you believe ZAAP will outperform the S&P 500 over the long term. Vote \"Underperform\" if you believe ZAAP will underperform the S&P 500 over the long term. You may vote once every thirty days.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize as gensim_summarize \n",
    "\n",
    "def summarize(string,**kwargs):\n",
    "    try:\n",
    "        summarized = gensim_summarize(string,**kwargs)\n",
    "    except:\n",
    "        return string\n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vote \"Outperform\" if you believe ZAAP will outperform the S&P 500 over the long term.\\nVote \"Underperform\" if you believe ZAAP will underperform the S&P 500 over the long term.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egSummary = summarize(eg)\n",
    "egSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Preprocess text<br>\n",
    "\n",
    "- remove puntuation: base on package: string.\n",
    "- remove stop words: based on english stopwords\n",
    "##### Certain parts of English speech, like conjunctions (“for”, “or”) or the word “the” are meaningless to a topic model. These terms are called stop words and need to be removed from our token list.\n",
    "- remove lemmatization: use nltk WordNetLemmatizer\n",
    "##### Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Original : forms, New: form Original : as, New: a\n",
    "- remove stemmization: use nltk SnowballStemmer\n",
    "##### Stemming words is another common NLP technique to reduce topically similar words to their root. For example, “stemming,” “stemmer,” “stemmed,” all have similar meanings; stemming reduces those terms to “stem.” This is important for topic modeling, which would otherwise view those terms as separate entities and reduce their importance in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = eg\n",
    "text = text.translate(str.maketrans('', '',string.digits))\n",
    "    ### Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZAP together with its subsidiaries designs develops manufactures and sells electric and advanced technology vehicles in the United States and internationally The company offers electric alternative energy and fuel efficient automobiles and commercial vehicles motorcycles and scooters and other forms of personal transportation ZAP also markets its electric transportation products through its zapworldcom Website The company was formerly known as ZAPWORLDCOM and changed its name to ZAP in  ZAP was founded in  and is headquartered in Santa Rosa California\\n\\nMarketBeat Community Rating for ZAP OTCMKTS ZAAP\\n\\nCommunity Ranking  out of    Outperform Votes  Vote Outperform Underperform Votes  Vote Underperform Total Votes \\n\\nMarketBeats community ratings are surveys of what our community members think about ZAP and other stocks Vote Outperform if you believe ZAAP will outperform the SP  over the long term Vote Underperform if you believe ZAAP will underperform the SP  over the long term You may vote once every thirty days'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text,return_str=False):\n",
    "    ### Remove number: for func `translate`: yourstring.translate(str.maketrans(fromstr, tostr, deletestr))\n",
    "    text = text.translate(str.maketrans('', '',string.digits))\n",
    "    ### Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    ### Remove stops words\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english') and not word.startswith(\"http\")]\n",
    "\n",
    "    ### Remove lemmatization\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    text = list(map(lambda x:wnl.lemmatize(x),text))\n",
    "    ### Remove stemmization\n",
    "#         stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    words = list(map(lambda x:stemmer.stem(x),text))\n",
    "    \n",
    "    if return_str:\n",
    "        return (' ').join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText = pre_process(egSummary,return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText = pre_process(egSummary,return_str=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vote',\n",
       " 'outperform',\n",
       " 'believ',\n",
       " 'zaap',\n",
       " 'outperform',\n",
       " 'SP',\n",
       " 'long',\n",
       " 'term',\n",
       " 'vote',\n",
       " 'underperform',\n",
       " 'believ',\n",
       " 'zaap',\n",
       " 'underperform',\n",
       " 'SP',\n",
       " 'long',\n",
       " 'term']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Modularize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypipe = ProcessPipeline(texts=[eg]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPipeline:\n",
    "\tdef __init__(self,texts=None,steps=[\"langdetection\",\"summarization\",'tokenization']):\n",
    "\t    self.stemmer = PorterStemmer()\n",
    "\t    self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "\t    self.texts = texts\n",
    "\t    self.steps = steps\n",
    "\n",
    "\tdef process(self,text,return_str=False):\n",
    "\t\tif \"langdetection\" in self.steps:\n",
    "\t\t\tlang = self.detect_lang(text)\n",
    "\t\t\tif lang == \"en\":\n",
    "\t\t\t\ttext =  text\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext = \"\"\n",
    "\t\tif \"summarization\"\tin self.steps:\n",
    "\t\t\ttext = self.summarize(text)\n",
    "\t\tif \"tokenization\" in self.steps:\n",
    "\t\t\tprocessed = self.pre_process(text,return_str=return_str)\n",
    "\t\t\treturn processed\n",
    "\t\telse:\n",
    "\t\t\treturn text\n",
    "\n",
    "\tdef run(self,return_str=False,workers=6):\n",
    "\t    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "\t    \tif return_str:\n",
    "\t        \tres = executor.map(self.process, self.texts,[True]*len(self.texts))        \t\t\n",
    "\t    \telse:\n",
    "\t        \tres = executor.map(self.process, self.texts)\n",
    "\t    return list(res)    \n",
    "\t        \n",
    "\tdef run_lambda(self):\n",
    "\t    return list(map(self.process,self.texts))\n",
    "\n",
    "\tdef run_loop(self):\n",
    "\t    processed = []\n",
    "\t    for i in self.texts:\n",
    "\t        processed.append(self.process(i))\n",
    "\t    return processed\n",
    "\n",
    "\tdef detect_lang(self,text):\n",
    "\t    ### translate to english\n",
    "\t    try:\n",
    "\t        language = detect(text)\n",
    "\t        print(f\"language is {language}\")\n",
    "\t    except:\n",
    "\t        print(\"Not able to detect language\")\n",
    "\t        language = \"other\"\n",
    "\t    return language\n",
    "\n",
    "\tdef summarize(self,text,**kwargs):\n",
    "\t    try:\n",
    "\t        summarized = gensim_summarize(text,**kwargs)\n",
    "\t        return summarized\n",
    "\t    except:\n",
    "\t        return text\n",
    "\n",
    "\tdef pre_process(self,text,return_str=False):\n",
    "\t    ### Remove number: for func `translate`: yourstring.translate(str.maketrans(fromstr, tostr, deletestr))\n",
    "\t    text = text.translate(str.maketrans('', '',string.digits))\n",
    "\t    ### Remove punctuation\n",
    "\t    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\t    ### Remove stops words\n",
    "\t    text = [word for word in text.split() if word.lower() not in stopwords.words('english') and not word.startswith(\"http\")]\n",
    "\n",
    "\t    ### Remove lemmatization\n",
    "\t    text = list(map(lambda x:self.lemmatizer.lemmatize(x),text))\n",
    "\t    ### Remove stemmization\n",
    "\t#         stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\t    words = list(map(lambda x:self.stemmer.stem(x),text))\n",
    "\t    if return_str:\n",
    "\t        return (' ').join(words)\n",
    "\t    else:\n",
    "\t        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ProcessPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language is en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egText = eg\n",
    "lang = pipeline.detect_lang(egText)\n",
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vote \"Outperform\" if you believe ZAAP will outperform the S&P 500 over the long term.\\nVote \"Underperform\" if you believe ZAAP will underperform the S&P 500 over the long term.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized = pipeline.summarize(egText)\n",
    "\n",
    "summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vote outperform believ zaap outperform SP long term vote underperform believ zaap underperform SP long term'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = pipeline.pre_process(summarized,return_str=True)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vote outperform believ zaap outperform SP long term vote underperform believ zaap underperform SP long term'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.pre_process(summarized,return_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Scrape raw news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDf = pd.read_csv(\"outputs/step1_bigquery_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(map(lambda x:get_text(x),rawDf.DocumentIdentifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save as pickle\n",
    "with open('outputs/step2_news_raw.pickle', 'wb') as handle:\n",
    "    pickle.dump(texts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
